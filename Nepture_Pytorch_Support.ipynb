{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNATmNt6/pY2WO42EnWm46N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hackathorn/WILS-MNIST/blob/main/Nepture_Pytorch_Support.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecD4zvkTypFb"
      },
      "outputs": [],
      "source": [
        "import neptune\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from neptune.utils import stringify_unsupported\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "-# Step 1: Initialize Neptune and create new Neptune run\n",
        "run = neptune.init_run(\n",
        "    project=\"common/pytorch-integration\",\n",
        "    tags=\"Basic script\",\n",
        "    api_token=neptune.ANONYMOUS_API_TOKEN,\n",
        "    source_files=[\"*.py\"],\n",
        ")\n",
        "\n",
        "# Experiment Config\n",
        "data_dir = \"data/CIFAR10\"\n",
        "compressed_ds = \"./data/CIFAR10/cifar-10-python.tar.gz\"\n",
        "data_tfms = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"lr\": 1e-2,\n",
        "    \"bs\": 128,\n",
        "    \"input_sz\": 32 * 32 * 3,\n",
        "    \"n_classes\": 10,\n",
        "    \"model_filename\": \"basemodel\",\n",
        "}\n",
        "\n",
        "\n",
        "# Model & Dataset\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_dim, n_classes):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_sz, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.view(-1, 32 * 32 * 3)\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "trainset = datasets.CIFAR10(data_dir, transform=data_tfms[\"train\"], download=True)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=params[\"bs\"], shuffle=True, num_workers=0\n",
        ")\n",
        "dataset_size = {\"train\": len(trainset)}\n",
        "\n",
        "# Instatiate model, criterion and optimizer\n",
        "model = BaseModel(params[\"input_sz\"], params[\"input_sz\"], params[\"n_classes\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=params[\"lr\"])\n",
        "\n",
        "# Step 2: Log config & pararameters\n",
        "run[\"config/dataset/path\"] = data_dir\n",
        "run[\"config/dataset/transforms\"] = stringify_unsupported(data_tfms)\n",
        "run[\"config/dataset/size\"] = dataset_size\n",
        "run[\"config/params\"] = stringify_unsupported(params)\n",
        "\n",
        "# Step 3: Log losses & metrics\n",
        "for i, (x, y) in enumerate(trainloader, 0):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(x)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    loss = criterion(outputs, y)\n",
        "    acc = (torch.sum(preds == y.data)) / len(x)\n",
        "\n",
        "    # Log batch loss\n",
        "    run[\"training/batch/loss\"].append(loss)\n",
        "\n",
        "    # Log batch accuracy\n",
        "    run[\"training/batch/acc\"].append(acc)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    }
  ]
}