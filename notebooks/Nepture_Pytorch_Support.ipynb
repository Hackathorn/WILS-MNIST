{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0sV07RdSpMvjGrm2Ddo3k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hackathorn/WILS-MNIST/blob/main/notebooks/Nepture_Pytorch_Support.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nepture"
      ],
      "metadata": {
        "id": "ashvIyle2-qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecD4zvkTypFb"
      },
      "outputs": [],
      "source": [
        "import neptune\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from neptune.utils import stringify_unsupported\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize Neptune and create new Neptune run\n",
        "run = neptune.init_run(\n",
        "    project=\"hack-richard/WILS-MNIST-Nepture-Pytorch-Support\",\n",
        "    tags=\"Basic script\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vbmV3LXVpLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9uZXctdWkubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMjAxNTIyOC01NWJmLTRmYmYtYmUzMC1mZTFlMWE1MGVhODgifQ==\",\n",
        "    source_files=[\"*.py\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "ysZRTTxX1hUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment Config\n",
        "data_dir = \"data/CIFAR10\"\n",
        "compressed_ds = \"./data/CIFAR10/cifar-10-python.tar.gz\"\n",
        "data_tfms = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "Qv_4LzbO1j7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"lr\": 1e-2,\n",
        "    \"bs\": 128,\n",
        "    \"input_sz\": 32 * 32 * 3,\n",
        "    \"n_classes\": 10,\n",
        "    \"model_filename\": \"basemodel\",\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qV2Jwr8k1mSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model & Dataset\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_dim, n_classes):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_sz, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.view(-1, 32 * 32 * 3)\n",
        "        return self.main(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "--8vgh1s1pSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.CIFAR10(data_dir, transform=data_tfms[\"train\"], download=True)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=params[\"bs\"], shuffle=True, num_workers=0\n",
        ")\n",
        "dataset_size = {\"train\": len(trainset)}\n",
        "\n"
      ],
      "metadata": {
        "id": "evYR8Mbq1td4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instatiate model, criterion and optimizer\n",
        "model = BaseModel(params[\"input_sz\"], params[\"input_sz\"], params[\"n_classes\"])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=params[\"lr\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "arxSkqW61wC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Log config & pararameters\n",
        "run[\"config/dataset/path\"] = data_dir\n",
        "run[\"config/dataset/transforms\"] = stringify_unsupported(data_tfms)\n",
        "run[\"config/dataset/size\"] = dataset_size\n",
        "run[\"config/params\"] = stringify_unsupported(params)\n",
        "\n"
      ],
      "metadata": {
        "id": "xCSt80eD1y4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Log losses & metrics\n",
        "for i, (x, y) in enumerate(trainloader, 0):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(x)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    loss = criterion(outputs, y)\n",
        "    acc = (torch.sum(preds == y.data)) / len(x)\n",
        "\n",
        "    # Log batch loss\n",
        "    run[\"training/batch/loss\"].append(loss)\n",
        "\n",
        "    # Log batch accuracy\n",
        "    run[\"training/batch/acc\"].append(acc)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "iCpZv5PV12JA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}